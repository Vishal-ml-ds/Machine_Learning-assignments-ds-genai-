{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef086a23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Question 1\n",
    "\n",
    "**What is a Decision Tree, and how does it work in the context of classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77df0440",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "A Decision Tree is a supervised machine learning model used for both classification and regression tasks. In classification, it splits the dataset into subsets based on feature values, creating a tree-like structure where each node represents a feature, each branch a decision rule, and each leaf a class label. It works by recursively selecting the best attribute to split the data using impurity measures such as Gini Impurity or Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88208d93",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "**Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2286c3f2",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Gini Impurity and Entropy are impurity measures used to evaluate splits in Decision Trees.  \n",
    "- **Gini Impurity** measures the likelihood of incorrect classification of a randomly chosen element.  \n",
    "- **Entropy** measures the level of uncertainty or disorder in the dataset.  \n",
    "Lower impurity leads to better splits. The decision tree algorithm selects features that result in the greatest reduction in impurity, improving classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f787569e",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "**What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d09f21",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "**Pre-Pruning** halts tree growth early by setting conditions like max depth or minimum samples per split.  \n",
    "**Post-Pruning** removes branches from a fully grown tree to reduce overfitting.  \n",
    "- Advantage of Pre-Pruning: Saves computation time and prevents over-complex models.  \n",
    "- Advantage of Post-Pruning: Results in simpler models with better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a49954",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "**What is Information Gain in Decision Trees, and why is it important for choosing the best split?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6dbef",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Information Gain measures the decrease in entropy after a dataset is split on a feature.  \n",
    "It is calculated as the difference between the entropy before the split and the weighted sum of entropy after the split.  \n",
    "Features with higher information gain are preferred, as they better separate the data and reduce impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595964e4",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "**What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ebc62d",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "**Common applications** include:  \n",
    "- Medical diagnosis  \n",
    "- Credit risk assessment  \n",
    "- Customer segmentation  \n",
    "\n",
    "**Advantages**: Easy to understand and visualize, handles both numerical and categorical data.  \n",
    "**Limitations**: Prone to overfitting, sensitive to small data changes, and less accurate compared to ensemble methods like Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43472805",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "**Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to handle the missing values, encode the categorical features, train a Decision Tree model, tune its hyperparameters, evaluate its performance, and describe what business value this model could provide in the real-world setting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9d2b00",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "1. **Handle missing values**:  \n",
    "   - Use `SimpleImputer` or drop rows/columns depending on missingness.  \n",
    "2. **Encode categorical features**:  \n",
    "   - Use `OneHotEncoder` or `LabelEncoder`.  \n",
    "3. **Train model**:  \n",
    "   - Use `DecisionTreeClassifier` on processed data.  \n",
    "4. **Tune hyperparameters**:  \n",
    "   - Use `GridSearchCV` or `RandomizedSearchCV`.  \n",
    "5. **Evaluate performance**:  \n",
    "   - Use accuracy, precision, recall, F1-score.  \n",
    "\n",
    "**Business Value**:  \n",
    "   - Enables early disease detection, reduces healthcare costs, assists in patient triage, and supports clinical decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8005ab",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "**Load the Iris Dataset, train a Decision Tree Classifier using the Gini criterion, print the model’s accuracy and feature importances.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7bb2f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Feature Importances: [0.01911002 0.01911002 0.53816374 0.42361622]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Feature Importances:\", clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aeac4c",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "**Load the Iris Dataset, train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab1c026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Tree Accuracy: 1.0\n",
      "Max Depth=3 Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "clf1 = DecisionTreeClassifier()\n",
    "clf1.fit(X_train, y_train)\n",
    "full_accuracy = accuracy_score(y_test, clf1.predict(X_test))\n",
    "\n",
    "clf2 = DecisionTreeClassifier(max_depth=3)\n",
    "clf2.fit(X_train, y_train)\n",
    "limited_accuracy = accuracy_score(y_test, clf2.predict(X_test))\n",
    "\n",
    "print(\"Full Tree Accuracy:\", full_accuracy)\n",
    "print(\"Max Depth=3 Accuracy:\", limited_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef5a019",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "**Load the Boston Housing Dataset, train a Decision Tree Regressor, and print the Mean Squared Error (MSE) and feature importances.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a445431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.5369178160347868\n",
      "Feature Importances: [0.52256426 0.05331831 0.05044119 0.02578106 0.03138394 0.13862404\n",
      " 0.09027145 0.08761574]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Decision Tree Regressor\n",
    "reg = DecisionTreeRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Print MSE and feature importances\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Feature Importances:\", reg.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecad53e",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "**Load the Iris Dataset, tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV, and print the best parameters and resulting accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52074a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 2, 'min_samples_split': 4}\n",
      "Best Accuracy: 0.9238095238095237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris Dataset\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Set hyperparameter grid\n",
    "params = {\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 4, 6]\n",
    "}\n",
    "\n",
    "# Grid search with 3-fold CV\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), params, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and accuracy\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Accuracy:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed467e66-0568-4433-8615-7325933b56bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3679c1e0-a4e3-4feb-bb7f-ca8469c11b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
