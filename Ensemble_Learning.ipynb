{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning: Assignment Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the complete answers for the assignment on Ensemble Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble Learning** is a machine learning technique where multiple individual models, often called \"base learners\" or \"weak learners,\" are strategically combined to produce a single, more powerful predictive model, or \"ensemble.\"\n",
    "\n",
    "\n",
    "\n",
    "The key idea behind it is rooted in the concept of **\"wisdom of the crowd.\"** The core principle is that a collective decision made by a diverse group of models is often more accurate, robust, and stable than a decision made by any single model. By aggregating the predictions of several models, ensemble methods can:\n",
    "\n",
    "\n",
    "\n",
    "- **Reduce Variance:** Techniques like Bagging help to minimize the model's sensitivity to small fluctuations in the training data, making it less prone to overfitting.\n",
    "\n",
    "- **Reduce Bias:** Techniques like Boosting aim to create a strong learner from a sequence of weak learners, progressively correcting errors and improving predictive accuracy.\n",
    "\n",
    "\n",
    "\n",
    "Ultimately, the goal is to create a final model that generalizes better to new, unseen data than any of its individual components could."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: What is the difference between Bagging and Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging (Bootstrap Aggregating)** and **Boosting** are two of the most popular ensemble learning techniques. While both combine multiple models, they do so in fundamentally different ways.\n",
    "\n",
    "| Feature | Bagging | Boosting |\n",
    "|---|---|---|\n",
    "| **Primary Goal** | To reduce the **variance** of a model. | To reduce the **bias** of a model. |\n",
    "| **Method** | Trains base models in **parallel**. Each model is trained independently on a different random subset of the data. | Trains base models **sequentially**. Each subsequent model is built to correct the errors of its predecessor. |\n",
    "| **Data Sampling** | Uses bootstrap sampling to create different training sets for each model. | The full dataset is used for each sequential model, but the weights of the data points are adjusted. Misclassified points are given higher weights. |\n",
    "| **Model Weighting** | All base models have an equal \"vote\" or contribution to the final prediction (e.g., through averaging or majority voting). | Models are weighted based on their performance. Better-performing models have a greater influence on the final prediction. |\n",
    "| **Model Dependence** | Base models are completely independent of each other. | Base models are dependent; the performance of one directly influences the training of the next. |\n",
    "| **Example Algorithms** | Random Forest, Bagging Classifiers/Regressors | AdaBoost, Gradient Boosting (GBM), XGBoost, LightGBM |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bootstrap sampling** is a resampling technique that involves repeatedly drawing random samples from an original dataset **with replacement**. This means that after a data point is selected for a sample, it is returned to the original dataset, making it eligible to be selected again.\n",
    "\n",
    "A key characteristic of bootstrap sampling is that each new sample will have the same size as the original dataset, but it will likely contain duplicate instances of some data points while completely omitting others. On average, a bootstrap sample will contain about **63.2%** of the unique instances from the original dataset.\n",
    "\n",
    "**Role in Bagging and Random Forest:**\n",
    "\n",
    "Bootstrap sampling is the foundational mechanism of Bagging. Its role is to **introduce diversity** among the base learners (e.g., decision trees in a Random Forest).\n",
    "\n",
    "1.  **Creating Diverse Training Sets:** In a Random Forest, instead of training all decision trees on the exact same data, each tree is trained on a different bootstrap sample. Since each sample is slightly different, the resulting trees are also different from one another.\n",
    "2.  **Reducing Variance and Overfitting:** High-variance models like decision trees tend to overfit their training data. By training many trees on different data subsets and then averaging their predictions, the Random Forest smooths out the individual errors and idiosyncrasies of each tree. This aggregation process significantly reduces the overall variance of the final model, making it more robust and better at generalizing to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Out-of-Bag (OOB) samples** are the data points from the original training set that were **not** included in a specific bootstrap sample used to train a particular base learner. Since bootstrap sampling draws data with replacement, some data points are selected multiple times while others (on average, about 36.8%) are not selected at all for a given sample. These left-out data points constitute the OOB set for that specific tree.\n",
    "\n",
    "**How OOB Score is Used for Evaluation:**\n",
    "\n",
    "The OOB samples act as a natural, built-in validation set for the ensemble model, allowing for performance evaluation without the need for a separate train-test split or cross-validation. The process works as follows:\n",
    "\n",
    "1.  **Prediction on OOB Samples:** For each data point in the original training set, a prediction is made using only the trees that did **not** see that data point during their training (i.e., the trees for which this data point was an OOB sample).\n",
    "2.  **Aggregation:** These predictions are aggregated (e.g., by majority vote for classification or averaging for regression) to form a final OOB prediction for that data point.\n",
    "3.  **Calculate OOB Score:** This process is repeated for every data point in the training set. The **OOB score** is then calculated by comparing all the OOB predictions against the true labels. It is typically an accuracy score for classification or an RÂ² score for regression.\n",
    "\n",
    "The OOB score provides a reliable and unbiased estimate of the model's performance on unseen data, making it a very convenient feature of Bagging methods like Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance analysis helps identify which features have the most predictive power. While both a single Decision Tree and a Random Forest can provide these scores, the way they are calculated and their reliability differ significantly.\n",
    "\n",
    "**Single Decision Tree:**\n",
    "- **Calculation:** The importance of a feature is calculated based on how much it reduces impurity (e.g., Gini impurity or entropy) each time it is used for a split. The total reduction in impurity caused by a feature across all its splits in the tree is its importance score.\n",
    "- **Limitations:** The importance scores from a single tree can be **unstable and have high variance**. A small change in the training data can lead to a completely different tree structure and, therefore, different importance scores. Furthermore, it can be biased towards features with high cardinality (many unique values).\n",
    "\n",
    "**Random Forest:**\n",
    "- **Calculation:** A Random Forest calculates the importance of a feature by **averaging its importance score across all the individual trees** in the forest. For each feature, its impurity reduction is calculated for every tree and then averaged. The final score is normalized so that the sum of all importances is 1.\n",
    "- **Advantages:** This averaging process makes the feature importance scores from a Random Forest much **more robust and reliable**. By aggregating the results from hundreds of different trees (each trained on a different subset of data and features), it mitigates the instability and bias of a single tree. The resulting scores provide a more stable and accurate reflection of a feature's true predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Write a Python program to load the Breast Cancer dataset, train a Random Forest Classifier, and print the top 5 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most important features based on Random Forest:\n",
      "                 Feature  Importance\n",
      "27  worst concave points    0.172295\n",
      "23            worst area    0.123192\n",
      "7    mean concave points    0.090299\n",
      "6         mean concavity    0.083215\n",
      "20          worst radius    0.081277\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "feature_names = cancer.feature_names\n",
    "\n",
    "# Split the data for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance scores\n",
    "importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Create a pandas DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort the features by importance and print the top 5\n",
    "top_5_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
    "\n",
    "print(\"Top 5 most important features based on Random Forest:\")\n",
    "print(top_5_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Write a Python program to train a Bagging Classifier using Decision Trees on the Iris dataset and compare its accuracy with a single Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of a Single Decision Tree: 1.0000\n",
      "Accuracy of the Bagging Classifier: 1.0000\n",
      "Out-of-Bag (OOB) Score of Bagging Classifier: 0.9429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 1. Train and evaluate a single Decision Tree\n",
    "single_tree = DecisionTreeClassifier(random_state=42)\n",
    "single_tree.fit(X_train, y_train)\n",
    "y_pred_tree = single_tree.predict(X_test)\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "print(f\"Accuracy of a Single Decision Tree: {accuracy_tree:.4f}\")\n",
    "\n",
    "# 2. Train and evaluate a Bagging Classifier with Decision Trees (Corrected)\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    oob_score=True\n",
    ")\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_clf.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "print(f\"Accuracy of the Bagging Classifier: {accuracy_bagging:.4f}\")\n",
    "print(f\"Out-of-Bag (OOB) Score of Bagging Classifier: {bagging_clf.oob_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Write a Python program to train a Random Forest Classifier, tune hyperparameters `max_depth` and `n_estimators` using GridSearchCV, and print the best parameters and final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': None, 'n_estimators': 100}\n",
      "Final Accuracy on Test Set: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Get the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Final Accuracy on Test Set: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Write a Python program to train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset and compare their Mean Squared Errors (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Bagging Regressor): 0.2568\n",
      "Mean Squared Error (Random Forest Regressor): 0.2565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 1. Train and evaluate a Bagging Regressor\n",
    "bagging_reg = BaggingRegressor(n_estimators=100, random_state=42)\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_reg.predict(X_test)\n",
    "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
    "print(f\"Mean Squared Error (Bagging Regressor): {mse_bagging:.4f}\")\n",
    "\n",
    "# 2. Train and evaluate a Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "y_pred_rf = rf_reg.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f\"Mean Squared Error (Random Forest Regressor): {mse_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Question 10: You are working as a data scientist at a financial institution to predict loan default... Explain your step-by-step approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting loan default is a critical, high-stakes task where accuracy and reliability are paramount. Using ensemble techniques is an excellent strategy. Here is a step-by-step approach I would take:\n",
    "\n",
    "#### 1. Choose Between Bagging or Boosting\n",
    "For this problem, my initial choice would be a **Boosting** algorithm, such as **XGBoost** or **LightGBM**. Here's why:\n",
    "- **Performance:** Loan default prediction is often a problem with complex interactions and an imbalanced class distribution. Boosting models excel at this by sequentially focusing on hard-to-classify cases, which often leads to higher predictive accuracy and better AUC scores compared to Bagging.\n",
    "- **Bias Reduction:** The primary goal is to minimize errors, especially false negatives (predicting 'no default' when it's a 'default'). Boosting's focus on reducing bias is well-suited for this objective.\n",
    "\n",
    "A **Random Forest (Bagging)** would be my strong secondary choice. It's more robust to noisy data, less prone to overfitting without extensive tuning, and its results are often easier to interpret. I would likely train both and compare their performance on key business metrics.\n",
    "\n",
    "#### 2. Handle Overfitting\n",
    "Overfitting is a major risk, especially with powerful models like XGBoost. My strategy would involve:\n",
    "- **Cross-Validation:** Use k-fold cross-validation (e.g., 5 or 10 folds) to get a reliable estimate of the model's performance on unseen data. For an imbalanced problem like loan default, **Stratified K-Fold** is essential to ensure each fold maintains the original class distribution.\n",
    "- **Hyperparameter Tuning:** Systematically tune key hyperparameters using `GridSearchCV` or `RandomizedSearchCV`. For XGBoost, this would include:\n",
    "  - `n_estimators`: The number of boosting rounds.\n",
    "  - `max_depth`: The maximum depth of each tree (to control complexity).\n",
    "  - `learning_rate`: A factor to shrink the contribution of each tree, preventing drastic updates.\n",
    "  - `subsample` and `colsample_bytree`: To add randomness, similar to Bagging.\n",
    "  - `gamma`, `lambda`, `alpha`: Regularization parameters to penalize model complexity.\n",
    "- **Early Stopping:** Monitor the model's performance on a dedicated validation set during training and stop the process when the performance metric (e.g., validation AUC) stops improving for a certain number of rounds.\n",
    "\n",
    "#### 3. Select Base Models\n",
    "The universally preferred base model for both Bagging and modern Boosting algorithms is the **Decision Tree**. The reasons are:\n",
    "- **Capability:** They can capture complex, non-linear relationships in the data.\n",
    "- **Speed:** They are computationally efficient to train.\n",
    "- **Weak Learner Principle:** In ensembles, particularly Boosting, the goal is to combine many \"weak learners\" (e.g., shallow decision trees with low `max_depth`) to create a single, powerful model. These simple trees have high bias but low variance, and the boosting process effectively reduces the overall bias.\n",
    "\n",
    "#### 4. Evaluate Performance using Cross-Validation\n",
    "As mentioned, I would use **Stratified K-Fold Cross-Validation**. The evaluation would focus on metrics suitable for imbalanced classification:\n",
    "- **AUC-ROC (Area Under the Receiver Operating Characteristic Curve):** The primary metric. It evaluates the model's ability to distinguish between the positive (default) and negative (no default) classes across all thresholds.\n",
    "- **Precision-Recall Curve (and AUPRC):** This is crucial when the positive class (default) is rare. It shows the trade-off between precision (the accuracy of positive predictions) and recall (the ability to find all positive instances).\n",
    "- **F1-Score:** The harmonic mean of precision and recall, providing a single score that balances both concerns.\n",
    "- **Confusion Matrix:** To analyze the specific types of errors being made, particularly the number of false negatives.\n",
    "\n",
    "#### 5. Justify How Ensemble Learning Improves Decision-Making\n",
    "In the context of loan defaults, using ensemble learning significantly improves decision-making in several ways:\n",
    "- **Higher Accuracy and Reduced Risk:** The primary benefit is a more accurate prediction model. A small increase in accuracy can translate into millions of dollars saved by correctly identifying loans that will default and avoiding those losses. \n",
    "- **Increased Stability and Reliability:** A single model might be sensitive to noise or specific patterns in the training data. An ensemble averages out these idiosyncrasies, leading to a much more stable model whose decisions are reliable and consistent when deployed.\n",
    "- **Better Business Insights:** Feature importance analysis from a robust ensemble model like Random Forest provides the institution with trustworthy insights into what factors (e.g., credit score, income-to-debt ratio, transaction patterns) are the most powerful predictors of default. This can inform not only individual loan decisions but also broader lending policies and risk management strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
